{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/space2/vsarathchandra/blockGPT/Results/FinalPickledResults/roc_pr_results_new_diffcast128.pkl', 'rb') as f:\n",
    "    results_diffcast_new = pickle.load(f)\n",
    "with open('/space2/vsarathchandra/blockGPT/Results/FinalPickledResults/roc_pr_results_new_nowcasting128.pkl', 'rb') as f:\n",
    "    results_nowcasting_new = pickle.load(f)\n",
    "with open('/space2/vsarathchandra/blockGPT/Results/FinalPickledResults/roc_pr_results_new_vqbatched128_temp03topk70.pkl', 'rb') as f:\n",
    "    results_vqbatched_new = pickle.load(f)\n",
    "with open('/space2/vsarathchandra/blockGPT/Results/FinalPickledResults/roc_pr_results_new_vqbatched128_temp03topk70_s2s3.pkl', 'rb') as f:\n",
    "    results_vqbatched_new_s2s3 = pickle.load(f)\n",
    "\n",
    "#change names\n",
    "results_diffcast_new = {'Diffcast+Phydnet': results_diffcast_new['diffcast_phydnet']}\n",
    "results_nowcasting_new = {'NowcastingGPT': results_nowcasting_new['vqgan_GPT']}\n",
    "results_vqbatched_new = {'BlockGPT': results_vqbatched_new['vqgan-GPT_batched']}\n",
    "results_vqbatched_new_s2 = {'BlockGPT': results_vqbatched_new_s2s3['vqgan_s2_GPT_batched']}\n",
    "results_vqbatched_new_s3 = {'BlockGPT': results_vqbatched_new_s2s3['vqgan_s3_GPT_batched']}\n",
    "\n",
    "with open ('/space2/vsarathchandra/blockGPT/Results/FinalPickledResults/roc_pr_results_new_diffcast128_s2.pkl', 'rb') as f:\n",
    "    results_diffcast_new_s2 = pickle.load(f)\n",
    "results_diffcast_new_s2 = {'Diffcast+Phydnet': results_diffcast_new_s2['diffcast_phydnet']}\n",
    "\n",
    "with open('/space2/vsarathchandra/blockGPT/Results/FinalPickledResults/roc_pr_results_new_nowcasting128_s3.pkl', 'rb') as f:\n",
    "    results_nowcasting_new_s3 = pickle.load(f)\n",
    "\n",
    "results_nowcasting_new_s3 = {'NowcastingGPT': results_nowcasting_new_s3['vqgan_s3_GPT']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_path is:  /space2/vsarathchandra/blockGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluate_catchment import average_region_metrics, get_overall_metriscs\n",
    "gt_thresholds=[1,2,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics_vqbatched_s1 = average_region_metrics(results_vqbatched_new, gt_thresholds)\n",
    "avg_metrics_vqbatched_s2 = average_region_metrics(results_vqbatched_new_s2, gt_thresholds)\n",
    "avg_metrics_vqbatched_s3 = average_region_metrics(results_vqbatched_new_s3, gt_thresholds)\n",
    "avg_merics_diffcasts1 = average_region_metrics(results_diffcast_new, gt_thresholds)\n",
    "avg_merics_diffcasts2 = average_region_metrics(results_diffcast_new_s2, gt_thresholds)\n",
    "avg_nowcasting = average_region_metrics(results_nowcasting_new, gt_thresholds)\n",
    "avg_nowcasting_s3 = average_region_metrics(results_nowcasting_new_s3, gt_thresholds)\n",
    "\n",
    "\n",
    "overall_metrics_vqbatched_s1 = get_overall_metriscs(results_vqbatched_new)\n",
    "overall_metrics_vqbatched_s2 = get_overall_metriscs(results_vqbatched_new_s2)\n",
    "overall_metrics_vqbatched_s3 = get_overall_metriscs(results_vqbatched_new_s3)\n",
    "overall_metrics_diffcasts1 = get_overall_metriscs(results_diffcast_new)\n",
    "overall_metrics_diffcasts2 = get_overall_metriscs(results_diffcast_new_s2)\n",
    "overall_nowcasting = get_overall_metriscs(results_nowcasting_new)\n",
    "overall_nowcasting_s3 = get_overall_metriscs(results_nowcasting_new_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "def average_catchment_results(seed_dicts):\n",
    "    \"\"\"\n",
    "    Averages per-time precision, recall, fpr, tpr, auc_roc, and auc_pr across multiple seeds.\n",
    "\n",
    "    Args:\n",
    "        seed_dicts (List[Dict]): List of result dictionaries (one per seed).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Averaged dictionary structure across seeds.\n",
    "    \"\"\"\n",
    "    # Use the first dict as a template\n",
    "    template = seed_dicts[0]\n",
    "    averaged_results = {}\n",
    "\n",
    "    for model in template:\n",
    "        averaged_results[model] = {}\n",
    "        for threshold in template[model]:\n",
    "            averaged_results[model][threshold] = {\"per_time\": []}\n",
    "            \n",
    "            # Assuming 9 time steps\n",
    "            for t in range(len(template[model][threshold][\"per_time\"])):\n",
    "                time_metrics = defaultdict(list)\n",
    "\n",
    "                for seed in seed_dicts:\n",
    "                    data = seed[model][threshold][\"per_time\"][t]\n",
    "                    for key, value in data.items():\n",
    "                        time_metrics[key].append(np.array(value))\n",
    "\n",
    "                averaged_time_metrics = {\n",
    "                    key: np.mean(np.stack(values, axis=0), axis=0)\n",
    "                    for key, values in time_metrics.items()\n",
    "                }\n",
    "\n",
    "                averaged_results[model][threshold][\"per_time\"].append(averaged_time_metrics)\n",
    "    \n",
    "    return averaged_results\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def average_overall_results(seed_dicts):\n",
    "    \"\"\"\n",
    "    Computes mean and std for per-time metrics across seeds.\n",
    "\n",
    "    Args:\n",
    "        seed_dicts (List[Dict]): List of result dictionaries (one per seed).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Nested dict structure with averaged and std per-time metrics.\n",
    "    \"\"\"\n",
    "    template = seed_dicts[0]\n",
    "    averaged_results = {}\n",
    "\n",
    "    for model in template:\n",
    "        averaged_results[model] = {}\n",
    "        for threshold in template[model]:\n",
    "            averaged_results[model][threshold] = {\"per_time\": []}\n",
    "\n",
    "            num_time_steps = len(template[model][threshold][\"per_time\"])\n",
    "            for t in range(num_time_steps):\n",
    "                time_metrics = defaultdict(list)\n",
    "\n",
    "                for seed in seed_dicts:\n",
    "                    data = seed[model][threshold][\"per_time\"][t]\n",
    "                    for key, value in data.items():\n",
    "                        time_metrics[key].append(np.array(value))\n",
    "\n",
    "                averaged_time_metrics = {}\n",
    "                for key, values in time_metrics.items():\n",
    "                    stacked = np.stack(values, axis=0)  # (num_seeds, len_thresholds) or scalar\n",
    "                    if stacked.ndim == 2:  # array metrics (precision, recall, etc.)\n",
    "                        if key != 'confusion':\n",
    "                            averaged_time_metrics[key] = np.mean(stacked, axis=0)\n",
    "              \n",
    "                    else:  # scalar metrics (auc_roc, auc_pr)\n",
    "                        if key != 'confusion':\n",
    "                            averaged_time_metrics[key] = float(np.mean(stacked))\n",
    "                        \n",
    "\n",
    "                averaged_results[model][threshold][\"per_time\"].append(averaged_time_metrics)\n",
    "\n",
    "    return averaged_results\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def average_seed_results_with_std(seed_dicts):\n",
    "    \"\"\"\n",
    "    Computes mean and std for per-time metrics across seeds.\n",
    "\n",
    "    Args:\n",
    "        seed_dicts (List[Dict]): List of result dictionaries (one per seed).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Nested dict structure with averaged and std per-time metrics.\n",
    "    \"\"\"\n",
    "    template = seed_dicts[0]\n",
    "    averaged_results = {}\n",
    "\n",
    "    for model in template:\n",
    "        averaged_results[model] = {}\n",
    "        for threshold in template[model]:\n",
    "            averaged_results[model][threshold] = {\"per_time\": []}\n",
    "\n",
    "            num_time_steps = len(template[model][threshold][\"per_time\"])\n",
    "            for t in range(num_time_steps):\n",
    "                time_metrics = defaultdict(list)\n",
    "\n",
    "                for seed in seed_dicts:\n",
    "                    data = seed[model][threshold][\"per_time\"][t]\n",
    "                    for key, value in data.items():\n",
    "                        time_metrics[key].append(np.array(value))\n",
    "\n",
    "                averaged_time_metrics = {}\n",
    "                for key, values in time_metrics.items():\n",
    "                    stacked = np.stack(values, axis=0)  # (num_seeds, len_thresholds) or scalar\n",
    "                    if stacked.ndim == 2:  # array metrics (precision, recall, etc.)\n",
    "                        if key != 'confusion':\n",
    "                            averaged_time_metrics[key + \"_mean\"] = np.mean(stacked, axis=0)\n",
    "                            averaged_time_metrics[key + \"_std\"] = np.std(stacked, axis=0)\n",
    "                    else:  # scalar metrics (auc_roc, auc_pr)\n",
    "                        if key != 'confusion':\n",
    "                            averaged_time_metrics[key + \"_mean\"] = float(np.mean(stacked))\n",
    "                            averaged_time_metrics[key + \"_std\"] = float(np.std(stacked))\n",
    "\n",
    "                averaged_results[model][threshold][\"per_time\"].append(averaged_time_metrics)\n",
    "\n",
    "    return averaged_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_seed_results_vq = average_seed_results_with_std([avg_metrics_vqbatched_s1, avg_metrics_vqbatched_s2, avg_metrics_vqbatched_s3])\n",
    "average_seed_results_diffcast = average_seed_results_with_std([avg_merics_diffcasts1, avg_merics_diffcasts2])\n",
    "avergare_seed_results_nowcasting = average_seed_results_with_std([avg_nowcasting, avg_nowcasting_s3])\n",
    "overall_seed_results_vq = average_seed_results_with_std([overall_metrics_vqbatched_s1, overall_metrics_vqbatched_s2, overall_metrics_vqbatched_s3])\n",
    "overall_seed_results_diffcast = average_seed_results_with_std([overall_metrics_diffcasts1, overall_metrics_diffcasts2])\n",
    "overall_seed_results_nowcasting = average_seed_results_with_std([overall_nowcasting, overall_nowcasting_s3])\n",
    "combined_results_avg = {**average_seed_results_vq,**average_seed_results_diffcast, **avergare_seed_results_nowcasting}\n",
    "combined_overall_results = {**overall_seed_results_vq, **overall_seed_results_diffcast, **overall_seed_results_nowcasting}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I plot the auc of roc and pr curves over time. In this plot, I show the standard deviation over the seeds as well. In the later plots, I show each pr and roc curve at every time instant. I could not figure out how to show standard deviation properly without messing up the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import plotly.io as pio\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def to_rgba(color_str, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Convert a hex or rgb string to rgba string with specified alpha.\n",
    "    \"\"\"\n",
    "    rgba = mcolors.to_rgba(color_str, alpha)\n",
    "    return f'rgba({int(rgba[0]*255)}, {int(rgba[1]*255)}, {int(rgba[2]*255)}, {rgba[3]:.2f})'\n",
    "\n",
    "def plot_auc_metrics_consistent_style(\n",
    "    avg_results: Dict[str, dict],\n",
    "    gt_thresholds: List[float],\n",
    "    output_folder: str,\n",
    "    context_length: int = 3,\n",
    "    segment_length: int = 9,\n",
    "    time_resolution: int = 30\n",
    "):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    model_names = list(avg_results.keys())\n",
    "    palette = px.colors.qualitative.Plotly\n",
    "    color_map = {\n",
    "        model: palette[i % len(palette)]\n",
    "        for i, model in enumerate(model_names)\n",
    "    }\n",
    "\n",
    "    time_steps = list(range(context_length, segment_length))\n",
    "    x_axis_labels = [f\"{(i + 1) * time_resolution - 90}\" for i in time_steps]\n",
    "\n",
    "    for metric in [\"auc_roc\", \"auc_pr\"]:\n",
    "        fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=len(gt_thresholds),\n",
    "            shared_yaxes=False,\n",
    "            subplot_titles=[f\"GT ≥ {gt} mm\" for gt in gt_thresholds]\n",
    "        )\n",
    "\n",
    "        for col, gt_thresh in enumerate(gt_thresholds, start=1):\n",
    "            for model in model_names:\n",
    "                if gt_thresh not in avg_results[model]:\n",
    "                    continue\n",
    "\n",
    "                per_time = avg_results[model][gt_thresh][\"per_time\"]\n",
    "                mean_vals = [step[f\"{metric}_mean\"] for step in per_time][context_length:]\n",
    "                std_vals = [step[f\"{metric}_std\"] for step in per_time][context_length:]\n",
    "\n",
    "                lower_bound = [max(m - s, 0) for m, s in zip(mean_vals, std_vals)]\n",
    "                upper_bound = [min(m + s, 1) for m, s in zip(mean_vals, std_vals)]\n",
    "\n",
    "                # Plot shaded region\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_axis_labels + x_axis_labels[::-1],\n",
    "                        y=upper_bound + lower_bound[::-1],\n",
    "                        fill='toself',\n",
    "                        fillcolor=to_rgba(color_map[model], alpha=0.2),\n",
    "                        line=dict(color='rgba(255,255,255,0)'),\n",
    "                        hoverinfo=\"skip\",\n",
    "                        showlegend=False,\n",
    "                        legendgroup=model\n",
    "                    ),\n",
    "                    row=1, col=col\n",
    "                )\n",
    "\n",
    "                # Plot mean line\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_axis_labels,\n",
    "                        y=mean_vals,\n",
    "                        mode=\"lines+markers\",\n",
    "                        name=model,\n",
    "                        line=dict(color=color_map[model]),\n",
    "                        marker=dict(color=color_map[model]),\n",
    "                        showlegend=(col == 1),\n",
    "                        legendgroup=model\n",
    "                    ),\n",
    "                    row=1, col=col\n",
    "                )\n",
    "\n",
    "            fig.update_xaxes(title_text=\"Time (min)\", row=1, col=col)\n",
    "            fig.update_yaxes(\n",
    "                title_text=\"AUC PR\" if metric == \"auc_pr\" else \"AUC ROC\",\n",
    "                row=1, col=col\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"AUC of {'PR' if metric == 'auc_pr' else 'ROC'} over Time by GT Threshold\",\n",
    "            title_x=0.5,\n",
    "            height=400,\n",
    "            width=400 * len(gt_thresholds),\n",
    "            template=\"plotly_white\",\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                y=-0.2,\n",
    "                x=0.5,\n",
    "                xanchor=\"center\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_path = os.path.join(output_folder, f\"{metric}_over_time_{timestamp}.png\")\n",
    "        pio.write_image(fig, save_path, format=\"png\", scale=3)\n",
    "        print(f\"Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Results/MetricsPlots/AUC_over_time_plots/KNMI30/Catchments/auc_roc_over_time_20250806_173558.png\n",
      "Saved: Results/MetricsPlots/AUC_over_time_plots/KNMI30/Catchments/auc_pr_over_time_20250806_173558.png\n",
      "Saved: Results/MetricsPlots/AUC_over_time_plots/KNMI30/Overall/auc_roc_over_time_20250806_173559.png\n",
      "Saved: Results/MetricsPlots/AUC_over_time_plots/KNMI30/Overall/auc_pr_over_time_20250806_173559.png\n"
     ]
    }
   ],
   "source": [
    "plot_auc_metrics_consistent_style(combined_results_avg, gt_thresholds, output_folder=\"Results/MetricsPlots/AUC_over_time_plots/KNMI30/Catchments\")\n",
    "plot_auc_metrics_consistent_style(combined_overall_results, gt_thresholds, output_folder=\"Results/MetricsPlots/AUC_over_time_plots/KNMI30/Overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the section which plots each roc and pr curve at every time instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_seed_results_vq = average_catchment_results([avg_metrics_vqbatched_s1, avg_metrics_vqbatched_s2, avg_metrics_vqbatched_s3])\n",
    "average_seed_results_diffcast = average_catchment_results([avg_merics_diffcasts1, avg_merics_diffcasts2])\n",
    "avergare_seed_results_nowcasting = average_catchment_results([avg_nowcasting, avg_nowcasting_s3])\n",
    "overall_seed_results_vq = average_overall_results([overall_metrics_vqbatched_s1, overall_metrics_vqbatched_s2, overall_metrics_vqbatched_s3])\n",
    "overall_seed_results_diffcast = average_overall_results([overall_metrics_diffcasts1, overall_metrics_diffcasts2])\n",
    "overall_seed_results_nowcasting = average_overall_results([overall_nowcasting, overall_nowcasting_s3])\n",
    "combined_results_avg = {**average_seed_results_vq,**average_seed_results_diffcast, **avergare_seed_results_nowcasting}\n",
    "combined_overall_results = {**overall_seed_results_vq, **overall_seed_results_diffcast, **overall_seed_results_nowcasting}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_time_sliced_curves_for_models_plotly_png(\n",
    "    avg_results: Dict[str, dict],\n",
    "    gt_thresholds: List[float],\n",
    "    metrics: List[str] = [\"roc\", \"pr\"],\n",
    "    prefix: str = \"\",\n",
    "    save_dir: str = \"./plots\"\n",
    "):\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model_names = list(avg_results.keys())\n",
    "    palette = px.colors.qualitative.Plotly\n",
    "    color_map = {model: palette[i % len(palette)] for i, model in enumerate(model_names)}\n",
    "\n",
    "    for metric in metrics:\n",
    "        assert metric in [\"roc\", \"pr\"], \"Metric must be 'roc' or 'pr'\"\n",
    "        metric_label = \"ROC\" if metric == \"roc\" else \"PR\"\n",
    "\n",
    "        for gt_thresh in gt_thresholds:\n",
    "            fig = make_subplots(\n",
    "                rows=2,\n",
    "                cols=3,\n",
    "                subplot_titles=[f\"{prefix}{metric_label} | Time = {30 * (t - 2)} min\" for t in range(3, 9)],\n",
    "                shared_xaxes=False,\n",
    "                shared_yaxes=False,\n",
    "                horizontal_spacing=0.1,\n",
    "                vertical_spacing=0.27  # increased spacing between top and bottom rows\n",
    "            )\n",
    "\n",
    "            for i, t in enumerate(range(3, 9)):\n",
    "                row, col = divmod(i, 3)\n",
    "                row += 1\n",
    "                col += 1\n",
    "                subplot_index = i + 1\n",
    "\n",
    "                x_label = \"False Positive Rate\" if metric == \"roc\" else \"Recall\"\n",
    "                y_label = \"True Positive Rate\" if metric == \"roc\" else \"Precision\"\n",
    "\n",
    "                x_min, x_max = float(\"inf\"), float(\"-inf\")\n",
    "                y_min, y_max = float(\"inf\"), float(\"-inf\")\n",
    "                auc_texts = []\n",
    "\n",
    "                for model in model_names:\n",
    "                    per_time = avg_results[model].get(gt_thresh, {}).get(\"per_time\", [])\n",
    "                    if t >= len(per_time):\n",
    "                        continue\n",
    "\n",
    "                    step = per_time[t]\n",
    "                    auc_val = step.get(\"auc_roc\" if metric == \"roc\" else \"auc_pr\", None)\n",
    "                    if auc_val is None:\n",
    "                        continue\n",
    "\n",
    "                    x_vals = step.get(\"fpr\" if metric == \"roc\" else \"recall\", [])\n",
    "                    y_vals = step.get(\"tpr\" if metric == \"roc\" else \"precision\", [])\n",
    "\n",
    "                    if len(x_vals) == 0 or len(y_vals) == 0:\n",
    "                        continue\n",
    "\n",
    "                    x_min = min(x_min, min(x_vals))\n",
    "                    x_max = max(x_max, max(x_vals))\n",
    "                    y_min = min(y_min, min(y_vals))\n",
    "                    y_max = max(y_max, max(y_vals))\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=x_vals,\n",
    "                            y=y_vals,\n",
    "                            mode=\"lines\",\n",
    "                            name=model,\n",
    "                            line=dict(color=color_map[model]),\n",
    "                            legendgroup=model,\n",
    "                            showlegend=(i == 0)\n",
    "                        ),\n",
    "                        row=row,\n",
    "                        col=col\n",
    "                    )\n",
    "\n",
    "                    auc_texts.append((model, auc_val))\n",
    "\n",
    "                x_margin = 0.05 * (x_max - x_min) if x_max > x_min else 0.01\n",
    "                y_margin = 0.05 * (y_max - y_min) if y_max > y_min else 0.01\n",
    "                fig.update_xaxes(title_text=x_label, range=[x_min - x_margin, x_max + x_margin], row=row, col=col)\n",
    "                fig.update_yaxes(title_text=y_label, range=[y_min - y_margin, y_max + y_margin], row=row, col=col)\n",
    "\n",
    "                # Safe AUC text positioning below x-axis labels\n",
    "                base_y = 0.55 if row == 1 else -0.08  # lower bottom row further\n",
    "                for j, (model, auc_val) in enumerate(auc_texts):\n",
    "                    fig.add_annotation(\n",
    "                        text=f\"{model}: AUC={auc_val:.3f}\",\n",
    "                        xref=f\"x{subplot_index}\",\n",
    "                        yref=\"paper\",\n",
    "                        x=(x_min + x_max) / 2,\n",
    "                        y=base_y - j * 0.04,\n",
    "                        xanchor=\"center\",\n",
    "                        yanchor=\"top\",\n",
    "                        showarrow=False,\n",
    "                        font=dict(size=10, color=color_map[model])\n",
    "                    )\n",
    "\n",
    "            fig.update_layout(\n",
    "                height=950,  # increased from 850\n",
    "                width=1100,\n",
    "                title_text=f\"{metric_label} Curves over Time (GT ≥ {gt_thresh} mm)\",\n",
    "                title_x=0.5,\n",
    "                template=\"plotly_white\",\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    y=-0.35,\n",
    "                    x=0.5,\n",
    "                    xanchor=\"center\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            filename = f\"{metric}_gt_{gt_thresh}mm_plotly.png\"\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            pio.write_image(fig, filepath, format=\"png\", scale=3)\n",
    "            print(f\"Saved: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Catchments/roc_gt_1mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Catchments/roc_gt_2mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Catchments/roc_gt_8mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Catchments/pr_gt_1mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Catchments/pr_gt_2mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Catchments/pr_gt_8mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Overall/roc_gt_1mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Overall/roc_gt_2mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Overall/roc_gt_8mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Overall/pr_gt_1mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Overall/pr_gt_2mm_plotly.png\n",
      "Saved: Results/MetricsPlots/ROC_PR_plots/KNMI30/Overall/pr_gt_8mm_plotly.png\n"
     ]
    }
   ],
   "source": [
    "plot_time_sliced_curves_for_models_plotly_png(combined_results_avg, gt_thresholds, prefix=\"Aggregated Catchment \", save_dir=\"Results/MetricsPlots/ROC_PR_plots/KNMI30/Catchments\")\n",
    "plot_time_sliced_curves_for_models_plotly_png(combined_overall_results, gt_thresholds, prefix=\"Aggregated Catchment \", save_dir=\"Results/MetricsPlots/ROC_PR_plots/KNMI30/Overall\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blockgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
