07/14/2025 13:35:23 - INFO - __main__ - ***** Running training *****
07/14/2025 13:35:23 - INFO - __main__ -   Num examples = 17750
07/14/2025 13:35:23 - INFO - __main__ -   Num Epochs = 20
07/14/2025 13:35:23 - INFO - __main__ -   Instantaneous batch size per device = 4
07/14/2025 13:35:23 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
07/14/2025 13:35:23 - INFO - __main__ -   Gradient Accumulation steps = 1
07/14/2025 13:35:23 - INFO - __main__ -   Total optimization steps = 2000000
  0%|                                                                                                                                                              | 0/2000000 [00:00<?, ?it/s]
model is:  GPT(
  (transformer): ModuleDict(
    (wte): Embedding(1024, 1024)
    (wpe): Embedding(1600, 1024)
    (drop): Dropout(p=0.0, inplace=False)
    (h): ModuleList(
      (0-15): 16 x Block(
        (ln_1): LayerNorm()
        (attn): CausalSelfAttention(
          (c_attn): Linear(in_features=1024, out_features=3072, bias=False)
          (c_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm()
        (mlp): MLP(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=False)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm()
  )
  (lm_head): Linear(in_features=1024, out_features=1024, bias=False)
)
Number of parameters in the model: 204047360
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
validation:   4%|█████▋                                                                                                                                        | 4/100 [00:06<01:33,  1.03it/s]
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
gen kwargs:  {'do_sample': True, 'temperature': 0.4, 'top_k': 70, 'max_new_tokens': 1280}
repeat iters 1
iter number 0
gen input shape is torch.Size([4, 320])
generated tokens shape in each generate iter is:  torch.Size([4, 1600])
results shape is  torch.Size([4, 1600])
largest token in generated tokens  tensor(1023, device='cuda:0')
indices shape torch.Size([4, 1600])
future length 20
largest index inside detokenize is  tensor(1023, device='cuda:0')
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
special token False
h shape torch.Size([100, 128, 8, 8])
entered no special token block
indices shape inside tokenizer torch.Size([4, 1600])
labels shape inside tokenizer torch.Size([4, 1600])
  File "/space2/vsarathchandra/blockGPT/train_gpt.py", line 867, in <module>
    start_train()
  File "/space2/vsarathchandra/blockGPT/train_gpt.py", line 798, in start_train
    _,loss = model(input_ids,labels)
  File "/space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/space2/vsarathchandra/blockGPT/models/blockGPT/model.py", line 439, in forward
    x = block(x)
  File "/space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/space2/vsarathchandra/blockGPT/models/blockGPT/model.py", line 332, in forward
    x = x + self.attn(self.ln_1(x))
  File "/space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/space2/vsarathchandra/conda_envs/blockgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/space2/vsarathchandra/blockGPT/models/blockGPT/model.py", line 261, in forward
    attn_mask = ~_get_base_mask_generation(self.block_size,self.block_size, x.device, self.num_block_tokens, True)
  File "/space2/vsarathchandra/blockGPT/models/blockGPT/model.py", line 91, in _get_base_mask_generation
    src_mask[tgt_index, :src_index] = False  # rows are targets, columns are sources            for tgt_index in range(tgt_length):
KeyboardInterrupt
