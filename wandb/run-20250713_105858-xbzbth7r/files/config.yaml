_wandb:
    value:
        cli_version: 0.19.1
        m: []
        python_version: 3.9.21
        t:
            "1":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 55
                - 71
                - 98
                - 105
            "2":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 55
                - 71
                - 98
                - 105
            "3":
                - 23
                - 55
            "4": 3.9.21
            "5": 0.19.1
            "6": 4.39.3
            "8":
                - 5
            "12": 0.19.1
            "13": linux-x86_64
action_conditioned:
    value: false
action_dim:
    value: 4
action_recon:
    value: null
checkpointing_steps:
    value: 5000
config_name:
    value: configs/GPT/config_blockGPT_KNMI30.json
context_length:
    value: 3
dataloader_num_workers:
    value: 8
dataset_name:
    value: knmi
dataset_path:
    value: /data2/frame_datasets
dataset_size:
    value: null
debug:
    value: false
embed_no_wd:
    value: true
encoder_config:
    value: configs/config_vqgan.json
eval_generate_times:
    value: 1
eval_only:
    value: false
exp_name:
    value: block64_8H_KNMI
goal_conditioned:
    value: false
gradient_accumulation_steps:
    value: 1
gradient_checkpointing:
    value: false
include_sos:
    value: false
latest_checkpoint_only:
    value: false
learning_rate:
    value: 1e-05
llama_attn_drop:
    value: 0.1
load_internal_llm:
    value: false
log_gif_interval:
    value: 10
log_steps:
    value: 100
lora:
    value: false
lora_alpha:
    value: 32
lora_dropout:
    value: 0
lora_r:
    value: 8
lr_scheduler_type:
    value: cosine
max_decode_batchsize:
    value: null
max_eval_iters:
    value: 100
max_generate_batchsize:
    value: null
max_grad_norm:
    value: null
max_train_steps:
    value: 2000000
mixed_precision:
    value: null
model_type:
    value: GPT
n_tokens_per_frame:
    value: 64
no_aug:
    value: false
num_train_epochs:
    value: 20
num_warmup_steps:
    value: 5000
output_dir:
    value: /projects/0/prjs0951/Varun/log_trm/2025-07-13-10:58:45-block64_8H_KNMI
oxe_data_mixes_type:
    value: select
per_device_eval_batch_size:
    value: 4
per_device_train_batch_size:
    value: 4
predictor_name:
    value: blockGPT
pretrained_model_name_or_path:
    value: /projects/0/prjs0951/Varun/Checkpoints/vqgan_knmi_7M_1024_ckpt-177500.pt
pretrained_transformer_path:
    value: null
report_to:
    value: wandb
resolution:
    value: 128
resume_from_checkpoint:
    value: null
reward_prediction:
    value: false
seed:
    value: 0
segment_length:
    value: 9
segment_length_sevir:
    value: 9
skip_first_val:
    value: false
special_token:
    value: false
start_completed_steps:
    value: null
sthsth_root_path:
    value: /data/something-something-v2/20bn-something-something-v2-frames-64
strong_aug:
    value: false
temp_res_sevir:
    value: 5
trust_remote_code:
    value: false
use_eval_dataset:
    value: true
validation_steps:
    value: 5000
video_stepsize:
    value: 1
vqgan_type:
    value: vqgan
weight_decay:
    value: 0.01
with_tracking:
    value: true
