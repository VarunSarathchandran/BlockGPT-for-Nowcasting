{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Functions\n",
    "#I used separate functions for each dataset initially. This was fixed later- but both are still included.\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "def to_rgba(color_str, alpha=0.2):\n",
    "    rgba = mcolors.to_rgba(color_str, alpha)\n",
    "    return f'rgba({int(rgba[0]*255)}, {int(rgba[1]*255)}, {int(rgba[2]*255)}, {rgba[3]:.2f})'\n",
    "\n",
    "def plot_metrics(results_tuple, output_folder, per_level_titles=None, x_axis_labels=None, context_length=3, segment_length=9, time_resolution=30):\n",
    "    per_level_results, aggregated_results, csi_far_results = results_tuple\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    model_names = list(per_level_results.keys())\n",
    "    palette = px.colors.qualitative.Plotly\n",
    "    color_map = {model: palette[i % len(palette)] for i, model in enumerate(model_names)}\n",
    "\n",
    "    time_steps = list(range(context_length, segment_length))\n",
    "    n_steps = len(time_steps)\n",
    "    if x_axis_labels is None:\n",
    "        x_axis_labels = [f\"{(i+1)*time_resolution}\" for i in range(n_steps)]\n",
    "\n",
    "    metrics = [\"MSE\", \"MAE\", \"PCC\"]\n",
    "    level_keys = list(per_level_results[model_names[0]].keys())\n",
    "    if per_level_titles is None:\n",
    "        per_level_titles = level_keys\n",
    "    title_map = dict(zip(level_keys, per_level_titles))\n",
    "\n",
    "    # ==== per-level MSE/MAE/PCC ====\n",
    "    for lvl in level_keys:\n",
    "        fig = make_subplots(rows=1, cols=3, shared_yaxes=False, subplot_titles=metrics)\n",
    "        for col, metric in enumerate(metrics, start=1):\n",
    "            for model in model_names:\n",
    "                mean_vals = per_level_results[model][lvl].get(metric + \"_mean\")\n",
    "                std_vals  = per_level_results[model][lvl].get(metric + \"_std\")\n",
    "\n",
    "                if mean_vals is not None and std_vals is not None:\n",
    "                    y_mean = [mean_vals[t] for t in time_steps]\n",
    "                    y_std = [std_vals[t] for t in time_steps]\n",
    "                    y_upper = [m + s for m, s in zip(y_mean, y_std)]\n",
    "                    y_lower = [m - s for m, s in zip(y_mean, y_std)]\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=x_axis_labels + x_axis_labels[::-1],\n",
    "                            y=y_upper + y_lower[::-1],\n",
    "                            fill='toself',\n",
    "                            fillcolor=to_rgba(color_map[model], alpha=0.2),\n",
    "                            line=dict(color='rgba(255,255,255,0)'),\n",
    "                            hoverinfo=\"skip\",\n",
    "                            showlegend=False\n",
    "                        ),\n",
    "                        row=1, col=col\n",
    "                    )\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=x_axis_labels,\n",
    "                            y=y_mean,\n",
    "                            mode=\"lines+markers\",\n",
    "                            name=model,\n",
    "                            line=dict(color=color_map[model]),\n",
    "                            marker=dict(color=color_map[model]),\n",
    "                            showlegend=(col == 1)\n",
    "                        ),\n",
    "                        row=1, col=col\n",
    "                    )\n",
    "            fig.update_xaxes(title_text=\"Time (min)\", row=1, col=col)\n",
    "            fig.update_yaxes(title_text=metric, row=1, col=col)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Metrics for Level {title_map[lvl]}\",\n",
    "            title_x=0.5,\n",
    "            legend=dict(orientation=\"h\", y=-0.2, x=0.5, xanchor=\"center\"),\n",
    "            height=400, width=1200\n",
    "        )\n",
    "        fig.write_image(os.path.join(output_folder, f\"metrics_level_{lvl}.png\"))\n",
    "\n",
    "    # ==== aggregated MSE/MAE/PCC ====\n",
    "    fig = make_subplots(rows=1, cols=3, shared_yaxes=False, subplot_titles=metrics)\n",
    "    for col, metric in enumerate(metrics, start=1):\n",
    "        for model in model_names:\n",
    "            mean_vals = aggregated_results[model].get(metric + \"_mean\")\n",
    "            std_vals = aggregated_results[model].get(metric + \"_std\")\n",
    "\n",
    "            if mean_vals is not None and std_vals is not None:\n",
    "                y_mean = [mean_vals[t] for t in time_steps]\n",
    "                y_std = [std_vals[t] for t in time_steps]\n",
    "                y_upper = [m + s for m, s in zip(y_mean, y_std)]\n",
    "                y_lower = [m - s for m, s in zip(y_mean, y_std)]\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_axis_labels + x_axis_labels[::-1],\n",
    "                        y=y_upper + y_lower[::-1],\n",
    "                        fill='toself',\n",
    "                        fillcolor=to_rgba(color_map[model], alpha=0.2),\n",
    "                        line=dict(color='rgba(255,255,255,0)'),\n",
    "                        hoverinfo=\"skip\",\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=1, col=col\n",
    "                )\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_axis_labels,\n",
    "                        y=y_mean,\n",
    "                        mode=\"lines+markers\",\n",
    "                        name=model,\n",
    "                        line=dict(color=color_map[model]),\n",
    "                        marker=dict(color=color_map[model]),\n",
    "                        showlegend=(col == 1)\n",
    "                    ),\n",
    "                    row=1, col=col\n",
    "                )\n",
    "        fig.update_xaxes(title_text=\"Time (min)\", row=1, col=col)\n",
    "        fig.update_yaxes(title_text=metric, row=1, col=col)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Aggregated Metrics\",\n",
    "        title_x=0.5,\n",
    "        legend=dict(orientation=\"h\", y=-0.2, x=0.5, xanchor=\"center\"),\n",
    "        height=400, width=1200\n",
    "    )\n",
    "    fig.write_image(os.path.join(output_folder, \"metrics_aggregated.png\"))\n",
    "\n",
    "    print(\"Saved all metric plots with standard deviation shading.\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def to_rgba(color_str, alpha=0.2):\n",
    "    rgba = mcolors.to_rgba(color_str, alpha)\n",
    "    return f'rgba({int(rgba[0]*255)}, {int(rgba[1]*255)}, {int(rgba[2]*255)}, {rgba[3]:.2f})'\n",
    "\n",
    "def plot_metrics_sevir(results_tuple, output_folder, per_level_titles=None, x_axis_labels=None,context_length=3,segment_length=9,time_resolution=30):\n",
    "    per_level_results, aggregated_results, csi_far_results = results_tuple\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    time_steps = list(range(context_length, segment_length))\n",
    "    n_steps = len(time_steps)\n",
    "    if x_axis_labels is None:\n",
    "        x_axis_labels = [f\"{(i+1)*time_resolution}\" for i in range(n_steps)]\n",
    "\n",
    "    model_names = list(per_level_results.keys())\n",
    "    palette = px.colors.qualitative.Plotly\n",
    "    color_map = {m: palette[i % len(palette)] for i,m in enumerate(model_names)}\n",
    "    legend_layout = dict(orientation=\"h\", y=-0.2, x=0.5, xanchor=\"center\")\n",
    "\n",
    "    levels = list(per_level_results[model_names[0]].keys())\n",
    "    if per_level_titles is None:\n",
    "        per_level_titles = levels\n",
    "    title_map = dict(zip(levels, per_level_titles))\n",
    "    metrics = [\"MSE\",\"MAE\",\"PCC\"]\n",
    "\n",
    "    # Per-level MSE/MAE/PCC\n",
    "    for lvl in levels:\n",
    "        fig = make_subplots(rows=1, cols=3, shared_yaxes=False, subplot_titles=metrics)\n",
    "        for col, metric in enumerate(metrics, start=1):\n",
    "            for model in model_names:\n",
    "                mean_vals = per_level_results[model][lvl].get(metric + \"_mean\")\n",
    "                std_vals = per_level_results[model][lvl].get(metric + \"_std\")\n",
    "                if mean_vals is not None and std_vals is not None:\n",
    "                    y_mean = [mean_vals[t] for t in time_steps]\n",
    "                    y_std = [std_vals[t] for t in time_steps]\n",
    "                    y_upper = [m + s for m, s in zip(y_mean, y_std)]\n",
    "                    y_lower = [m - s for m, s in zip(y_mean, y_std)]\n",
    "                    fig.add_trace(go.Scatter(x=x_axis_labels + x_axis_labels[::-1], y=y_upper + y_lower[::-1], fill='toself', fillcolor=to_rgba(color_map[model], 0.2), line=dict(color='rgba(255,255,255,0)'), showlegend=False), row=1, col=col)\n",
    "                    fig.add_trace(go.Scatter(x=x_axis_labels, y=y_mean, mode=\"lines+markers\", name=model, line=dict(color=color_map[model]), marker=dict(color=color_map[model]), showlegend=(col==1)), row=1, col=col)\n",
    "            fig.update_xaxes(title_text=\"Time (min)\", row=1, col=col)\n",
    "            fig.update_yaxes(title_text=metric, row=1, col=col)\n",
    "\n",
    "        fig.update_layout(title_text=f\"Metrics for Level {title_map[lvl]}\", title_x=0.5, legend=legend_layout, height=400, width=1200)\n",
    "        fig.write_image(os.path.join(output_folder, f\"metrics_level_{lvl}.png\"))\n",
    "\n",
    "    # Aggregated MSE/MAE/PCC\n",
    "    fig = make_subplots(rows=1, cols=3, shared_yaxes=False, subplot_titles=metrics)\n",
    "    for col, metric in enumerate(metrics, start=1):\n",
    "        for model in model_names:\n",
    "            mean_vals = aggregated_results[model].get(metric + \"_mean\")\n",
    "            std_vals = aggregated_results[model].get(metric + \"_std\")\n",
    "            if mean_vals is not None and std_vals is not None:\n",
    "                y_mean = [mean_vals[t] for t in time_steps]\n",
    "                y_std = [std_vals[t] for t in time_steps]\n",
    "                y_upper = [m + s for m, s in zip(y_mean, y_std)]\n",
    "                y_lower = [m - s for m, s in zip(y_mean, y_std)]\n",
    "                fig.add_trace(go.Scatter(x=x_axis_labels + x_axis_labels[::-1], y=y_upper + y_lower[::-1], fill='toself', fillcolor=to_rgba(color_map[model], 0.2), line=dict(color='rgba(255,255,255,0)'), showlegend=False), row=1, col=col)\n",
    "                fig.add_trace(go.Scatter(x=x_axis_labels, y=y_mean, mode=\"lines+markers\", name=model, line=dict(color=color_map[model]), marker=dict(color=color_map[model]), showlegend=(col==1)), row=1, col=col)\n",
    "        fig.update_xaxes(title_text=\"Time (min)\", row=1, col=col)\n",
    "        fig.update_yaxes(title_text=metric, row=1, col=col)\n",
    "\n",
    "    fig.update_layout(title_text=\"Aggregated Metrics Across All Levels\", title_x=0.5, legend=legend_layout, height=400, width=1200)\n",
    "    fig.write_image(os.path.join(output_folder, \"metrics_aggregated.png\"))\n",
    "\n",
    "    # CSI and FAR: 2x3 grid\n",
    "    thresholds = list(csi_far_results[model_names[0]].keys())\n",
    "    assert len(thresholds) == 6, \"Expected 6 thresholds for 2×3 layout\"\n",
    "    for metric in [\"CSI\", \"FAR\"]:\n",
    "        fig = make_subplots(rows=2, cols=3, shared_yaxes=False, subplot_titles=[f\"{thr}\" for thr in thresholds])\n",
    "        for idx, thr in enumerate(thresholds):\n",
    "            r, c = divmod(idx, 3)\n",
    "            for model in model_names:\n",
    "                mean_vals = csi_far_results[model][thr].get(f\"{metric}_mean\")\n",
    "                std_vals = csi_far_results[model][thr].get(f\"{metric}_std\")\n",
    "                if mean_vals is not None and std_vals is not None:\n",
    "                    y_mean = [mean_vals[t] for t in time_steps]\n",
    "                    y_std = [std_vals[t] for t in time_steps]\n",
    "                    y_upper = [m + s for m, s in zip(y_mean, y_std)]\n",
    "                    y_lower = [m - s for m, s in zip(y_mean, y_std)]\n",
    "                    fig.add_trace(go.Scatter(x=x_axis_labels + x_axis_labels[::-1], y=y_upper + y_lower[::-1], fill='toself', fillcolor=to_rgba(color_map[model], 0.2), line=dict(color='rgba(255,255,255,0)'), showlegend=False), row=r+1, col=c+1)\n",
    "                    fig.add_trace(go.Scatter(x=x_axis_labels, y=y_mean, mode=\"lines+markers\", name=model, line=dict(color=color_map[model]), marker=dict(color=color_map[model]), showlegend=(idx==0)), row=r+1, col=c+1)\n",
    "            fig.update_xaxes(title_text=\"Time (min)\", row=r+1, col=c+1)\n",
    "            fig.update_yaxes(title_text=metric, row=r+1, col=c+1)\n",
    "\n",
    "        fig.update_layout(title_text=f\"{metric} at Various Thresholds\", title_x=0.5, legend=legend_layout, height=800, width=1200)\n",
    "        fig.write_image(os.path.join(output_folder, f\"{metric.lower()}_2x3.png\"))\n",
    "\n",
    "        import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Global parameters (reuse from earlier)\n",
    "\n",
    "\n",
    "def plot_metrics_relative_sevir(results_tuple, ref_model, output_folder,\n",
    "                          per_level_titles=None, x_axis_labels=None,context_length=3,segment_length=9,time_resolution=30):\n",
    "    \"\"\"\n",
    "    Plots percentage difference of each model relative to a reference model\n",
    "    for MSE, MAE, PCC, CSI, and FAR. Uses the same layout and color scheme\n",
    "    as plot_metrics_sevir, but with dashed lines and zero-baseline.\n",
    "\n",
    "    Args:\n",
    "        results_tuple (tuple):\n",
    "            - per_level_results: dict[model][level][\"MSE\"/\"MAE\"/\"PCC\"] -> list over time\n",
    "            - aggregated_results: dict[model][\"MSE\"/\"MAE\"/\"PCC\"] -> list over time\n",
    "            - csi_far_results: dict[model][threshold][\"CSI\"/\"FAR\"] -> list over time\n",
    "        ref_model (str): Name of the reference model to compare against.\n",
    "        output_folder (str): Directory to save figures.\n",
    "        per_level_titles (list): Display names for each precipitation level.\n",
    "        x_axis_labels (list): Labels (minutes) for the predicted time steps.\n",
    "    \"\"\"\n",
    "    per_level_results, aggregated_results, csi_far_results = results_tuple\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # build color map identical to before\n",
    "    model_names = list(per_level_results.keys())\n",
    "    palette = px.colors.qualitative.Plotly\n",
    "    color_map = {m: palette[i % len(palette)] for i, m in enumerate(model_names)}\n",
    "\n",
    "    # time steps and labels\n",
    "    time_steps = list(range(context_length, segment_length))\n",
    "    n_steps = len(time_steps)\n",
    "    if x_axis_labels is None:\n",
    "        x_axis_labels = [f\"{(i+1)*time_resolution}\" for i in range(n_steps)]\n",
    "\n",
    "    # horizontal zero baseline trace\n",
    "    zero_line = dict(mode=\"lines\", line=dict(color=\"black\", width=1, dash=\"dot\"), showlegend=False)\n",
    "\n",
    "    # common legend layout\n",
    "    legend_layout = dict(orientation=\"h\", y=-0.2, x=0.5, xanchor=\"center\")\n",
    "\n",
    "    metrics = [\"MSE\", \"MAE\", \"PCC\"]\n",
    "    levels = list(per_level_results[ref_model].keys())\n",
    "    if per_level_titles is None:\n",
    "        per_level_titles = levels\n",
    "    title_map = dict(zip(levels, per_level_titles))\n",
    "\n",
    "    # 1) Per-level MSE/MAE/PCC relative plots\n",
    "    for lvl in levels:\n",
    "        fig = make_subplots(rows=1, cols=3, shared_yaxes=False, subplot_titles=metrics)\n",
    "        for col, metric in enumerate(metrics, start=1):\n",
    "            ref_vals = per_level_results[ref_model][lvl][metric]\n",
    "            for model in model_names:\n",
    "                if model == ref_model:\n",
    "                    continue\n",
    "                vals = per_level_results[model][lvl][metric]\n",
    "                y = [ (vals[t] - ref_vals[t]) / ref_vals[t] * 100 for t in time_steps ]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=x_axis_labels, y=y,\n",
    "                               mode=\"lines+markers\",\n",
    "                               name=model,\n",
    "                               line=dict(color=color_map[model], dash=\"dash\"),\n",
    "                               marker=dict(color=color_map[model]),\n",
    "                               showlegend=(col==1)\n",
    "                              ),\n",
    "                    row=1, col=col\n",
    "                )\n",
    "            # add zero baseline\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=[0]*n_steps, **zero_line), row=1, col=col)\n",
    "            fig.update_xaxes(title_text=\"Time (min)\", row=1, col=col)\n",
    "            fig.update_yaxes(title_text=f\"% Δ {metric}\", row=1, col=col)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"% Difference vs {ref_model} | Level {title_map[lvl]}\",\n",
    "            title_x=0.5, legend=legend_layout,\n",
    "            height=400, width=1200\n",
    "        )\n",
    "        fig.write_image(os.path.join(output_folder, f\"rel_metrics_level_{lvl}.png\"))\n",
    "      #  fig.close()\n",
    "\n",
    "    # 2) Aggregated MSE/MAE/PCC relative\n",
    "    fig = make_subplots(rows=1, cols=3, shared_yaxes=False, subplot_titles=metrics)\n",
    "    for col, metric in enumerate(metrics, start=1):\n",
    "        ref_vals = aggregated_results[ref_model][metric]\n",
    "        for model in model_names:\n",
    "            if model == ref_model:\n",
    "                continue\n",
    "            vals = aggregated_results[model][metric]\n",
    "            y = [ (vals[t] - ref_vals[t]) / ref_vals[t] * 100 for t in time_steps ]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=x_axis_labels, y=y,\n",
    "                           mode=\"lines+markers\",\n",
    "                           name=model,\n",
    "                           line=dict(color=color_map[model], dash=\"dash\"),\n",
    "                           marker=dict(color=color_map[model]),\n",
    "                           showlegend=(col==1)\n",
    "                          ),\n",
    "                row=1, col=col\n",
    "            )\n",
    "        fig.add_trace(go.Scatter(x=x_axis_labels, y=[0]*n_steps, **zero_line), row=1, col=col)\n",
    "        fig.update_xaxes(title_text=\"Time (min)\", row=1, col=col)\n",
    "        fig.update_yaxes(title_text=f\"% Δ {metric}\", row=1, col=col)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"% Difference vs {ref_model} | Aggregated\",\n",
    "        title_x=0.5, legend=legend_layout,\n",
    "        height=400, width=1200\n",
    "    )\n",
    "    fig.write_image(os.path.join(output_folder, \"rel_metrics_aggregated.png\"))\n",
    "   # fig.close()\n",
    "\n",
    "    # 3) CSI: 2×3 grid relative\n",
    "    thresholds = list(csi_far_results[ref_model].keys())\n",
    "    assert len(thresholds) == 6, \"Expected 6 thresholds for 2×3 layout\"\n",
    "    fig = make_subplots(rows=2, cols=3, shared_yaxes=False,\n",
    "                        subplot_titles=[f\"{thr}\" for thr in thresholds])\n",
    "    for idx, thr in enumerate(thresholds):\n",
    "        r, c = divmod(idx, 3)\n",
    "        ref_vals = csi_far_results[ref_model][thr][\"CSI\"]\n",
    "        for model in model_names:\n",
    "            if model == ref_model:\n",
    "                continue\n",
    "            vals = csi_far_results[model][thr][\"CSI\"]\n",
    "            y = [ (vals[t] - ref_vals[t]) / ref_vals[t] * 100 for t in time_steps ]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=x_axis_labels, y=y,\n",
    "                           mode=\"lines+markers\",\n",
    "                           name=model,\n",
    "                           line=dict(color=color_map[model], dash=\"dash\"),\n",
    "                           marker=dict(color=color_map[model]),\n",
    "                           showlegend=(idx==0)\n",
    "                          ),\n",
    "                row=r+1, col=c+1\n",
    "            )\n",
    "        fig.add_trace(go.Scatter(x=x_axis_labels, y=[0]*n_steps, **zero_line), row=r+1, col=c+1)\n",
    "        fig.update_xaxes(title_text=\"Time (min)\", row=r+1, col=c+1)\n",
    "        fig.update_yaxes(title_text=\"% Δ CSI\", row=r+1, col=c+1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"% Difference CSI vs {ref_model}\",\n",
    "        title_x=0.5, legend=legend_layout,\n",
    "        height=800, width=1200\n",
    "    )\n",
    "    fig.write_image(os.path.join(output_folder, \"rel_csi_2x3.png\"))\n",
    "   # fig.close()\n",
    "\n",
    "    # 4) FAR: 2×3 grid relative\n",
    "    fig = make_subplots(rows=2, cols=3, shared_yaxes=False,\n",
    "                        subplot_titles=[f\"{thr}\" for thr in thresholds])\n",
    "    for idx, thr in enumerate(thresholds):\n",
    "        r, c = divmod(idx, 3)\n",
    "        ref_vals = csi_far_results[ref_model][thr][\"FAR\"]\n",
    "        for model in model_names:\n",
    "            if model == ref_model:\n",
    "                continue\n",
    "            vals = csi_far_results[model][thr][\"FAR\"]\n",
    "            y = [ (vals[t] - ref_vals[t]) / ref_vals[t] * 100 for t in time_steps ]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=x_axis_labels, y=y,\n",
    "                           mode=\"lines+markers\",\n",
    "                           name=model,\n",
    "                           line=dict(color=color_map[model], dash=\"dash\"),\n",
    "                           marker=dict(color=color_map[model]),\n",
    "                           showlegend=(idx==0)\n",
    "                          ),\n",
    "                row=r+1, col=c+1\n",
    "            )\n",
    "        fig.add_trace(go.Scatter(x=x_axis_labels, y=[0]*n_steps, **zero_line), row=r+1, col=c+1)\n",
    "        fig.update_xaxes(title_text=\"Time (min)\", row=r+1, col=c+1)\n",
    "        fig.update_yaxes(title_text=\"% Δ FAR\", row=r+1, col=c+1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"% Difference FAR vs {ref_model}\",\n",
    "        title_x=0.5, legend=legend_layout,\n",
    "        height=800, width=1200\n",
    "    )\n",
    "    fig.write_image(os.path.join(output_folder, \"rel_far_2x3.png\"))\n",
    "  #  fig.close()\n",
    "\n",
    "#averaging function across seeds. Also calcs std dev.\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def average_nested_lists_across_seeds_std(seed_results):\n",
    "    \"\"\"\n",
    "    Averages nested metric lists across multiple seed result tuples and computes standard deviation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_results : list of tuples\n",
    "        Each tuple has the structure: (level_metrics, overall_metrics, threshold_metrics)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Averaged version of (level_metrics, overall_metrics, threshold_metrics)\n",
    "        Each metric now includes both _mean and _std\n",
    "    \"\"\"\n",
    "    def average_leaf_lists(dicts):\n",
    "        \"\"\"Averages leaf lists in a list of identically structured dicts and computes std.\"\"\"\n",
    "        result = {}\n",
    "        for key in dicts[0]:\n",
    "            if isinstance(dicts[0][key], dict):\n",
    "                result[key] = average_leaf_lists([d[key] for d in dicts])\n",
    "            elif isinstance(dicts[0][key], list):\n",
    "                stacked = np.stack([d[key] for d in dicts])  # shape (num_seeds, ...)\n",
    "                result[key + \"_mean\"] = stacked.mean(axis=0).tolist()\n",
    "                result[key + \"_std\"] = stacked.std(axis=0).tolist()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported type at key {key}: {type(dicts[0][key])}\")\n",
    "        return result\n",
    "\n",
    "    # Transpose seed_results into 3 lists: per_level_all, overall_all, threshold_all\n",
    "    per_level_all = [seed[0] for seed in seed_results]\n",
    "    overall_all   = [seed[1] for seed in seed_results]\n",
    "    threshold_all = [seed[2] for seed in seed_results]\n",
    "\n",
    "    # Averaging per-level metrics\n",
    "    per_level_avg = {}\n",
    "    for model in per_level_all[0]:\n",
    "        per_level_avg[model] = {}\n",
    "        for level in per_level_all[0][model]:\n",
    "            per_level_dicts = [seed[model][level] for seed in per_level_all]\n",
    "            per_level_avg[model][level] = average_leaf_lists(per_level_dicts)\n",
    "\n",
    "    # Averaging overall metrics\n",
    "    overall_avg = {}\n",
    "    for model in overall_all[0]:\n",
    "        model_dicts = [seed[model] for seed in overall_all]\n",
    "        overall_avg[model] = average_leaf_lists(model_dicts)\n",
    "\n",
    "    # Averaging threshold-based metrics\n",
    "    threshold_avg = {}\n",
    "    for model in threshold_all[0]:\n",
    "        threshold_avg[model] = {}\n",
    "        for thresh in threshold_all[0][model]:\n",
    "            thresh_dicts = [seed[model][thresh] for seed in threshold_all]\n",
    "            threshold_avg[model][thresh] = average_leaf_lists(thresh_dicts)\n",
    "\n",
    "    return (per_level_avg, overall_avg, threshold_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNMI 30 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "#BlockGPT\n",
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI30_seed1.pkl','rb') as file:\n",
    "    vqbatched = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI30_seed2seed3.pkl','rb') as file:\n",
    "    vqbatched_s2s3 = pickle.load(file)\n",
    "\n",
    "# Diffcast\n",
    "    \n",
    "with open('Results/FinalPickledResults/results_diffcast_phydnet_KNMI30_seed1.pkl','rb') as file:\n",
    "    diffcast_s1 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_diffcast_phydnet_KNMI30_seed2seed3.pkl','rb') as file:\n",
    "    diffcast_s2s3 = pickle.load(file)\n",
    "\n",
    "#NowcastingGPT\n",
    "    \n",
    "with open ('Results/FinalPickledResults/results_nowcastingGPT_KNMI30_seed1.pkl','rb') as file:\n",
    "    nowcasting_s1 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_KNMI30_seed2seed3.pkl','rb') as file:\n",
    "    nowcasting_s2s3 = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vqbatched_s1 = tuple({ 'BlockGPT': entry['vqgan-GPT_batched'] } for entry in vqbatched)\n",
    "vqbatched_s2 = tuple({ 'BlockGPT': entry['vqgan-s1_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "vqbatched_s3 = tuple({ 'BlockGPT': entry['vqgan-s2_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([vqbatched_s1,vqbatched_s2, vqbatched_s3])\n",
    "seed_results_vqbatched = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "\n",
    "diffcast_s1 = tuple({ 'Diffcast+Phydnet': entry['diffcast_phydnet'] } for entry in diffcast_s1)\n",
    "diffcast_s2 = tuple({ 'Diffcast+Phydnet': entry['diffcast_s1_phydnet'] } for entry in diffcast_s2s3)\n",
    "diffcast_s3 = tuple({ 'Diffcast+Phydnet': entry['diffcast_s2_phydnet'] } for entry in diffcast_s2s3)\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([diffcast_s1,diffcast_s2, diffcast_s3])\n",
    "seed_results_diffcast = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "\n",
    "nowcasting_s1= tuple({ 'NowcastingGPT': entry['vqgan_s1-GPT'] } for entry in nowcasting_s1)\n",
    "nowcasting_s2 = tuple({ 'NowcastingGPT': entry['vqgan_s2-GPT'] } for entry in nowcasting_s2s3)\n",
    "nowcasting_s3 = tuple({ 'NowcastingGPT': entry['vqgan_s3-GPT'] } for entry in nowcasting_s2s3)\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([nowcasting_s1,nowcasting_s2, nowcasting_s3])\n",
    "seed_results_nowcasting = (avg_per_level, avg_overall, avg_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results_seeds= tuple(\n",
    "    {**d1, **d2,**d3}  # Merge dictionaries at the same index\n",
    "    for d1, d2,d3 in zip(seed_results_vqbatched,seed_results_diffcast,seed_results_nowcasting)\n",
    ")\n",
    "\n",
    "\n",
    "plot_metrics(combined_results_seeds , \"Results/MetricsPlots/KNMI30\",[\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-95\",\"95-100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ablations on KNMI 30 \n",
    "#16H indicates 16 heads\n",
    "#block 8 represents a block size of 8- generates a row at a time\n",
    "#tt is token-by-token, rr is row  by row and ff is frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI30_seed1.pkl','rb') as file:\n",
    "    vqbatched = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_50M_200M.pkl','rb') as file:\n",
    "    vqbatched_4H_16H = pickle.load(file)\n",
    "\n",
    "with open('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_50M_200M_seed2seed3.pkl','rb') as file:\n",
    "    vqbatched_4H_16H_s2s3 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_rowbyrow_seed1.pkl','rb') as file:\n",
    "    vqbatched_block8 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_rowbyrow_seed2seed3.pkl','rb') as file:\n",
    "    vqbatched_block8_s2s3 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vqbatched_s1 = tuple({ 'BlockGPT_8H': entry['vqgan-GPT_batched'] } for entry in vqbatched)\n",
    "vqbatched_s2 = tuple({ 'BlockGPT_8H': entry['vqgan-s1_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "vqbatched_s3 = tuple({ 'BlockGPT_8H': entry['vqgan-s2_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "results_seeds = [vqbatched_s1,vqbatched_s2, vqbatched_s3]\n",
    "\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([vqbatched_s1,vqbatched_s2, vqbatched_s3])\n",
    "seed_results_vqbatched = (avg_per_level, avg_overall, avg_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename \n",
    "vqbatched_4H_s1 = tuple({ 'BlockGPT 4H': entry['vqgan_4H-GPT_batched'] } for entry in vqbatched_4H_16H)\n",
    "vqbatched_4H_s2 = tuple({ 'BlockGPT 4H': entry['vqgan_4H-s2_GPT_batched'] } for entry in vqbatched_4H_16H_s2s3)\n",
    "vqbatched_4H_s3 = tuple({ 'BlockGPT 4H': entry['vqgan_4H-s3_GPT_batched'] } for entry in vqbatched_4H_16H_s2s3)\n",
    "vqbatched_16H_s1 = tuple({ 'BlockGPT 16H': entry['vqgan_16H-GPT_batched'] } for entry in vqbatched_4H_16H)\n",
    "vqbatched_16H_s2 = tuple({ 'BlockGPT 16H': entry['vqgan_16H-s2_GPT_batched'] } for entry in vqbatched_4H_16H_s2s3)\n",
    "vqbatched_16H_s3 = tuple({ 'BlockGPT 16H': entry['vqgan_16H-s3_GPT_batched'] } for entry in vqbatched_4H_16H_s2s3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename NowcastingGPT to token-by-token in seed_results_nowcasting\n",
    "tt = tuple({ 'token-by-token (NowcastingGPT)': entry['NowcastingGPT'] } for entry in seed_results_nowcasting)\n",
    "rr = tuple({ 'row-by-row': entry['vqgan_block8-GPT_batched'] } for entry in vqbatched_block8)\n",
    "ff = tuple({ 'frame-by-frame (BlockGPT)': entry['BlockGPT_8H'] } for entry in seed_results_vqbatched)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg across seeds\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([vqbatched_4H_s1,vqbatched_4H_s2, vqbatched_4H_s3])\n",
    "seed_results_vqbatched_4H = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([vqbatched_16H_s1,vqbatched_16H_s2, vqbatched_16H_s3])\n",
    "seed_results_vqbatched_16H = (avg_per_level, avg_overall, avg_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rr_s2 = tuple({ 'row-by-row': entry['vqgan_block8-s2-GPT_batched'] } for entry in vqbatched_block8_s2s3)\n",
    "rr_s3 = tuple({ 'row-by-row': entry['vqgan_block8-s3-GPT_batched'] } for entry in vqbatched_block8_s2s3)\n",
    "\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([rr ,rr_s2, rr_s3])\n",
    "seed_results_vqbatched_rr = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "combined_results = tuple(\n",
    "    {**d1, **d2,**d3}  # Merge dictionaries at the same index\n",
    "    for d1, d2,d3 in zip(seed_results_vqbatched_4H,seed_results_vqbatched,seed_results_vqbatched_16H)\n",
    ")\n",
    "plot_metrics(combined_results , \"Results/FinalMetricPlots/head_size\",[\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-95\",\"95-100\"])\n",
    "\n",
    "combined_results = tuple(\n",
    "    {**d1, **d2,**d3}  # Merge dictionaries at the same index\n",
    "    for d1, d2,d3 in zip(tt,seed_results_vqbatched_rr,ff)\n",
    ")\n",
    "\n",
    "\n",
    "plot_metrics(combined_results , \"Results/FinalMetricPlots/KNMI30/Ablations/block_size\",[\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-95\",\"95-100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_winter_seeds123.pkl','rb') as file:\n",
    "    vqbatched_winter = pickle.load(file)\n",
    "#rename \n",
    "vqbatched_winter_s1 = tuple({ 'BlockGPT in winter': entry['vqgan-s1_GPT_batched'] } for entry in vqbatched_winter)\n",
    "vqbatched_winter_s2 = tuple({ 'BlockGPT in winter': entry['vqgan-s2_GPT_batched'] } for entry in vqbatched_winter)\n",
    "vqbatched_winter_s3 = tuple({ 'BlockGPT in winter': entry['vqgan-s3_GPT_batched'] } for entry in vqbatched_winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_spring_seeds123.pkl','rb') as file:\n",
    "    vqbatched_spring = pickle.load(file)\n",
    "#rename\n",
    "vqbatched_spring_s1 = tuple({ 'BlockGPT in spring': entry['vqgan-s1_GPT_batched'] } for entry in vqbatched_spring)\n",
    "vqbatched_spring_s2 = tuple({ 'BlockGPT in spring': entry['vqgan-s2_GPT_batched'] } for entry in vqbatched_spring)\n",
    "vqbatched_spring_s3 = tuple({ 'BlockGPT in spring': entry['vqgan-s3_GPT_batched'] } for entry in vqbatched_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summer\n",
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_summer_seeds123.pkl','rb') as file:\n",
    "    vqbatched_summer = pickle.load(file)\n",
    "#rename\n",
    "vqbatched_summer_s1 = tuple({ 'BlockGPT in summer': entry['vqgan-s1_GPT_batched'] } for entry in vqbatched_summer)\n",
    "vqbatched_summer_s2 = tuple({ 'BlockGPT in summer': entry['vqgan-s2_GPT_batched'] } for entry in vqbatched_summer)\n",
    "vqbatched_summer_s3 = tuple({ 'BlockGPT in summer': entry['vqgan-s3_GPT_batched'] } for entry in vqbatched_summer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fall\n",
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI30_ablatations_fall_seeds123.pkl','rb') as file:\n",
    "    vqbatched_fall = pickle.load(file)\n",
    "#rename\n",
    "vqbatched_fall_s1 = tuple({ 'BlockGPT in fall': entry['vqgan-s1_GPT_batched'] } for entry in vqbatched_fall)\n",
    "vqbatched_fall_s2 = tuple({ 'BlockGPT in fall': entry['vqgan-s2_GPT_batched'] } for entry in vqbatched_fall)\n",
    "vqbatched_fall_s3 = tuple({ 'BlockGPT in fall': entry['vqgan-s3_GPT_batched'] } for entry in vqbatched_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avergae across seeds\n",
    "vq_summmer = average_nested_lists_across_seeds_std([vqbatched_summer_s1,vqbatched_summer_s2,vqbatched_summer_s3])\n",
    "vq_winter = average_nested_lists_across_seeds_std([vqbatched_winter_s1,vqbatched_winter_s2,vqbatched_winter_s3])\n",
    "vq_spring = average_nested_lists_across_seeds_std([vqbatched_spring_s1,vqbatched_spring_s2,vqbatched_spring_s3])\n",
    "vq_fall = average_nested_lists_across_seeds_std([vqbatched_fall_s1,vqbatched_fall_s2,vqbatched_fall_s3])\n",
    "\n",
    "combined_results = tuple(\n",
    "    {**d1, **d2,**d3,**d4}  # Merge dictionaries at the same index\n",
    "    for d1, d2,d3,d4 in zip(vq_summmer,vq_winter,vq_spring,vq_fall)\n",
    ")\n",
    "\n",
    "#plot\n",
    "plot_metrics(combined_results, \"Results/FinalMetricPlots/KNMI30/Ablations/seasons\",[\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-95\",\"95-100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEVIR 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Results/FinalPickledResults/results_blockGPT_SEVIR30_seed2seed3.pkl','rb') as file:\n",
    "    vqbatched_s2s3 = pickle.load(file)\n",
    "\n",
    "with open('Results/FinalPickledResults/results_blockGPT_diffcast_phydnet_seed1.pkl','rb') as file:\n",
    "    diffcast_vqbatched_s1 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_diffcast_phydnet_SEVIR30_seed2seed3.pkl','rb') as file:\n",
    "    diffcast_s2s3 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_SEVIR30_seed1.pkl','rb') as file:\n",
    "    nowcasting_s1 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_SEVIR30_seed2seed3.pkl','rb') as file:\n",
    "    nowcasting_s2s3 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open('Results/FinalPickledResults/results_diffcast_blockGPT_SEVIR30_seed1.pkl','rb') as file:\n",
    "    diffcastBlockGPT = pickle.load(file)\n",
    "diffcastBlockGPT = tuple({ 'diffcast+BlockGPT': entry['diffcast_BlockGPT'] } for entry in diffcastBlockGPT)\n",
    "\n",
    "with open('Results/FinalPickledResults/results_diffcast_blockGPT_SEVIR30_seed2.pkl','rb') as file:\n",
    "    diffcastBlockGPT_seed2 = pickle.load(file)\n",
    "\n",
    "diffcastBlockGPT_seed2 = tuple({ 'diffcast+BlockGPT': entry['diffcast_BlockGPT'] } for entry in diffcastBlockGPT_seed2)\n",
    "\n",
    "with open('Results/FinalPickledResults/results_diffcast_blockGPT_SEVIR30_seed3.pkl','rb') as file:\n",
    "    diffcastBlockGPT_seed3 = pickle.load(file)\n",
    "diffcastBlockGPT_seed3 = tuple({ 'diffcast+BlockGPT': entry['diffcast_BlockGPT'] } for entry in diffcastBlockGPT_seed3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffcast_s1 = tuple({ 'Diffcast+Phydnet': entry['diffcast_phydnet'] } for entry in diffcast_vqbatched_s1)\n",
    "diffcast_s2 = tuple({ 'Diffcast+Phydnet': entry['diffcast_s1_phydnet'] } for entry in diffcast_s2s3)\n",
    "diffcast_s3 = tuple({ 'Diffcast+Phydnet': entry['diffcast_s2_phydnet'] } for entry in diffcast_s2s3)\n",
    "\n",
    "vqbatched_s1 = tuple({ 'BlockGPT': entry['vqgan-GPT_batched'] } for entry in diffcast_vqbatched_s1)\n",
    "vqbatched_s2 = tuple({ 'BlockGPT': entry['vqgan_s1_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "vqbatched_s3 = tuple({ 'BlockGPT': entry['vqgan_s2_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "\n",
    "nowcasting_s1 = tuple({ 'NowcastingGPT': entry['vqgan_s1-GPT'] } for entry in nowcasting_s1)\n",
    "nowcasting_s2 = tuple({ 'NowcastingGPT': entry['vqgan_s2-GPT'] } for entry in nowcasting_s2s3)\n",
    "nowcasting_s3 = tuple({ 'NowcastingGPT': entry['vqgan_s3-GPT'] } for entry in nowcasting_s2s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([diffcast_s1,diffcast_s2, diffcast_s3])\n",
    "seed_results_diffcast = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([vqbatched_s1,vqbatched_s2, vqbatched_s3])\n",
    "seed_results_vqbatched = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([nowcasting_s1,nowcasting_s2, nowcasting_s3])\n",
    "seed_results_nowcasting = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([diffcastBlockGPT, diffcastBlockGPT_seed2, diffcastBlockGPT_seed3])\n",
    "seed_results_diffblockgpt= (avg_per_level, avg_overall, avg_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results_sevir30 = tuple(\n",
    "    {**d1, **d2,**d3}  # Merge dictionaries at the same index\n",
    "    for d1, d2,d3 in zip(seed_results_vqbatched,seed_results_diffcast,seed_results_nowcasting)\n",
    ")\n",
    "\n",
    "plot_metrics(combined_results_sevir30 , \"Results/MetricsPlots/SEVIR30\",[\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-95\",\"95-100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sevir 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('Results/FinalPickledResults/results_blockGPT_SEVIR5_seed2seed3.pkl','rb') as file:\n",
    "    vqbatched_s2s3= pickle.load(file)#\n",
    "\n",
    "with open ('Results/FinalPickledResults/results_blockGPT_SEVIR5_seed1.pkl','rb') as file:\n",
    "    vqbatched_s1= pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_diffcast_phydnet_SEVIR5_seed2seed3.pkl','rb') as file:\n",
    "    diffcast_s2s3 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_diffcast_phydnet_SEVIR5_seed1.pkl','rb') as file:\n",
    "    diffcast_s1 = pickle.load(file)\n",
    "diffcast_s1 = tuple({ 'Diffcast+Phydnet': entry['diffcast_phydnet'] } for entry in diffcast_s1)\n",
    "\n",
    "\n",
    "\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_SEVIR5_seed1.pkl','rb') as file:\n",
    "    nowcasting_sevir_big_s1 = pickle.load(file)\n",
    "\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_SEVIR5_seed2.pkl','rb') as file:\n",
    "    nowcasting_sevir_big_s2 = pickle.load(file)\n",
    "\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_SEVIR5_seed3.pkl','rb') as file:\n",
    "    nowcasting_sevir_big_s3 = pickle.load(file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqbatched_s1 = tuple({ 'BlockGPT': entry['vqgan-GPT_batched'] } for entry in vqbatched_s1)\n",
    "vqbatched_s2 = tuple({ 'BlockGPT': entry['vqgan_s1_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "vqbatched_s3 = tuple({ 'BlockGPT': entry['vqgan_s2_GPT_batched'] } for entry in vqbatched_s2s3)\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([vqbatched_s1,vqbatched_s2, vqbatched_s3])\n",
    "seed_results_vqbatched = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "diffcast_s1 = tuple({ 'Diffcast+Phydnet': entry['diffcast_phydnet'] } for entry in diffcast_s1)\n",
    "diffcast_s2 = tuple({ 'Diffcast+Phydnet': entry['diffcast_s1_phydnet'] } for entry in diffcast_s2s3)\n",
    "diffcast_s3 = tuple({ 'Diffcast+Phydnet': entry['diffcast_s2_phydnet'] } for entry in diffcast_s2s3)\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([diffcast_s1,diffcast_s2, diffcast_s3])\n",
    "seed_results_diffcast= (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "nowcasting_s1 = tuple({ 'NowcastingGPT': entry['vqgan-s1_GPT'] } for entry in nowcasting_sevir_big_s1)\n",
    "nowcasting_s2 = tuple({ 'NowcastingGPT': entry['vqgan-s2_GPT'] } for entry in nowcasting_sevir_big_s2)\n",
    "nowcasting_s3 = tuple({ 'NowcastingGPT': entry['vqgan-s3_GPT'] } for entry in nowcasting_sevir_big_s3)\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([nowcasting_s1,nowcasting_s2, nowcasting_s3])\n",
    "seed_results_nowcasting = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "combined_metrics_sevir= tuple(\n",
    "    {**d1, **d2,**d3}  # Merge dictionaries at the same index\n",
    "    for d1, d2,d3 in zip(seed_results_vqbatched,seed_results_diffcast,seed_results_nowcasting)\n",
    ")\n",
    "plot_metrics(combined_metrics_sevir , \"Results/MetricsPlots/SEVIR5\",[\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-95\",\"95-100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNMI 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('Results/FinalPickledResults/results_blockGPT_KNMI5_seeds123.pkl','rb') as file:\n",
    "    vqbatched_s1s2s3 = pickle.load(file)\n",
    "    \n",
    "with open('Results/FinalPickledResults/results_diffcast_phydnet_KNMI5_seed1.pkl','rb') as file:\n",
    "     diffcast_s1 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_diffcast_phydnet_KNMI5_seed2seed3.pkl','rb') as file:\n",
    "    diffcast_s2s3 = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "with open ('Results/FinalPickledResults/results_nowcastingGPT_KNMI5_seed1.pkl','rb') as file:\n",
    "    nowcasting_s1 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_KNMI5_seed2.pkl','rb') as file:\n",
    "    nowcasting_s2 = pickle.load(file)\n",
    "with open('Results/FinalPickledResults/results_nowcastingGPT_KNMI5_seed3.pkl','rb') as file:\n",
    "    nowcasting_s3 = pickle.load(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vqbatched_s1 = tuple({ 'BlockGPT': entry['vqgan_GPT_batched'] } for entry in     vqbatched_s1s2s3)\n",
    "vqbatched_s2 = tuple({ 'BlockGPT': entry['vqgan_s2_GPT_batched'] } for entry in     vqbatched_s1s2s3)\n",
    "vqbatched_s3 = tuple({ 'BlockGPT': entry['vqgan_s3_GPT_batched'] } for entry in     vqbatched_s1s2s3)\n",
    "#avg vqbatched\n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([vqbatched_s1,vqbatched_s2, vqbatched_s3])\n",
    "seed_results_vqbatched = (avg_per_level, avg_overall, avg_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffcast_s1 = tuple({ 'Diffcast+Phydnet': entry['diffcast_phydnet'] } for entry in diffcast_s1)\n",
    "diffcast_s2 = tuple({ 'Diffcast+Phydnet': entry['diffcast_phydnet'] } for entry in diffcast_s2s3)\n",
    "diffcast_s3 = tuple({ 'Diffcast+Phydnet': entry['diffcast_s2_phydnet'] } for entry in diffcast_s2s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nowcasting_s1 = tuple({ 'NowcastingGPT': entry['vqgan-GPT'] } for entry in nowcasting_s1)\n",
    "nowcasting_s2 = tuple({ 'NowcastingGPT': entry['vqgan-s2_GPT'] } for entry in nowcasting_s2)\n",
    "nowcasting_s3 = tuple({ 'NowcastingGPT': entry['vqgan-s3_GPT'] } for entry in nowcasting_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg diffcast results \n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([diffcast_s1,diffcast_s2, diffcast_s3])\n",
    "seed_results_diffcast = (avg_per_level, avg_overall, avg_threshold)\n",
    "\n",
    "#avg nowcasting results \n",
    "avg_per_level, avg_overall, avg_threshold = average_nested_lists_across_seeds_std([nowcasting_s1,nowcasting_s2, nowcasting_s3])\n",
    "seed_results_nowcasting = (avg_per_level, avg_overall, avg_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_results = tuple(\n",
    "    {**d1, **d2,**d3}  # Merge dictionaries at the same index\n",
    "    for d1, d2,d3 in zip(seed_results_vqbatched,seed_results_diffcast,seed_results_nowcasting)\n",
    ")\n",
    "plot_metrics(combined_results , \"Results/MetricsPlots/SEVIR5\",[\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-95\",\"95-100\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivideogpt-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
